{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3550998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importation all necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6db0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "# Define the directories for training, validation, and testing datasets\n",
    "data_dir = 'flower_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25632b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for the training, validation, and testing sets\n",
    "# These transforms include resizing, cropping, normalizing, and converting to tensor\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "#  Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "validation_data = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir ,transform = test_transforms)\n",
    "\n",
    "# Define the dataloaders for training, validation, and testing datasets and load them in batches\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "vloader = torch.utils.data.DataLoader(validation_data, batch_size =32,shuffle = True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 20, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the category to name mapping\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78280a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=102, bias=True)\n",
       "    (4): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained resnet50 model with default weights\n",
    "model = models.resnet50(weights='DEFAULT')\n",
    "# Freeze parameters so we don't backprop through them during training \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Define our new classifier to train\n",
    "classifier = nn.Sequential(nn.Linear(2048, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(0.2),\n",
    "                           nn.Linear(512, 102),\n",
    "                           \n",
    "                           nn.LogSoftmax(dim=1))\n",
    "# Replace the model's classifier with our new classifier\n",
    "model.fc = classifier\n",
    "# Define the loss function \n",
    "criterion = nn.NLLLoss()\n",
    "# Define the optimizer\n",
    "# Only train the classifier parameters, feature parameters are frozen   \n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "# Move the model to the device (GPU or CPU)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1.. Train loss: 4.656.. Validation loss: 4.480.. Validation accuracy: 0.113\n",
      "Epoch 1/1.. Train loss: 4.395.. Validation loss: 4.281.. Validation accuracy: 0.152\n",
      "Epoch 1/1.. Train loss: 4.414.. Validation loss: 4.067.. Validation accuracy: 0.215\n",
      "Epoch 1/1.. Train loss: 4.132.. Validation loss: 3.803.. Validation accuracy: 0.258\n",
      "Epoch 1/1.. Train loss: 3.812.. Validation loss: 3.570.. Validation accuracy: 0.287\n",
      "Epoch 1/1.. Train loss: 3.904.. Validation loss: 3.305.. Validation accuracy: 0.328\n",
      "Epoch 1/1.. Train loss: 3.234.. Validation loss: 3.068.. Validation accuracy: 0.326\n",
      "Epoch 1/1.. Train loss: 3.167.. Validation loss: 2.813.. Validation accuracy: 0.376\n",
      "Epoch 1/1.. Train loss: 3.081.. Validation loss: 2.525.. Validation accuracy: 0.445\n",
      "Epoch 1/1.. Train loss: 2.979.. Validation loss: 2.344.. Validation accuracy: 0.482\n",
      "Epoch 1/1.. Train loss: 2.616.. Validation loss: 2.128.. Validation accuracy: 0.546\n",
      "Epoch 1/1.. Train loss: 2.633.. Validation loss: 1.967.. Validation accuracy: 0.556\n",
      "Epoch 1/1.. Train loss: 2.274.. Validation loss: 1.777.. Validation accuracy: 0.594\n",
      "Epoch 1/1.. Train loss: 2.256.. Validation loss: 1.542.. Validation accuracy: 0.643\n",
      "Epoch 1/1.. Train loss: 1.891.. Validation loss: 1.435.. Validation accuracy: 0.665\n",
      "Epoch 1/1.. Train loss: 1.619.. Validation loss: 1.442.. Validation accuracy: 0.651\n",
      "Epoch 1/1.. Train loss: 2.068.. Validation loss: 1.411.. Validation accuracy: 0.655\n",
      "Epoch 1/1.. Train loss: 2.006.. Validation loss: 1.324.. Validation accuracy: 0.655\n",
      "Epoch 1/1.. Train loss: 1.755.. Validation loss: 1.258.. Validation accuracy: 0.675\n",
      "Epoch 1/1.. Train loss: 1.878.. Validation loss: 1.137.. Validation accuracy: 0.698\n",
      "Epoch 1/1.. Train loss: 1.641.. Validation loss: 1.156.. Validation accuracy: 0.719\n",
      "Epoch 1/1.. Train loss: 1.401.. Validation loss: 1.034.. Validation accuracy: 0.746\n",
      "Epoch 1/1.. Train loss: 1.564.. Validation loss: 1.018.. Validation accuracy: 0.740\n",
      "Epoch 1/1.. Train loss: 1.545.. Validation loss: 1.020.. Validation accuracy: 0.728\n",
      "Epoch 1/1.. Train loss: 1.325.. Validation loss: 0.931.. Validation accuracy: 0.771\n",
      "Epoch 1/1.. Train loss: 1.463.. Validation loss: 0.888.. Validation accuracy: 0.759\n",
      "Epoch 1/1.. Train loss: 1.269.. Validation loss: 0.854.. Validation accuracy: 0.784\n",
      "Epoch 1/1.. Train loss: 1.274.. Validation loss: 0.900.. Validation accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 1.428.. Validation loss: 0.854.. Validation accuracy: 0.774\n",
      "Epoch 1/1.. Train loss: 1.474.. Validation loss: 0.817.. Validation accuracy: 0.780\n",
      "Epoch 1/1.. Train loss: 1.398.. Validation loss: 0.740.. Validation accuracy: 0.809\n",
      "Epoch 1/1.. Train loss: 1.201.. Validation loss: 0.832.. Validation accuracy: 0.780\n",
      "Epoch 1/1.. Train loss: 1.224.. Validation loss: 0.816.. Validation accuracy: 0.781\n",
      "Epoch 1/1.. Train loss: 1.222.. Validation loss: 0.702.. Validation accuracy: 0.826\n",
      "Epoch 1/1.. Train loss: 1.230.. Validation loss: 0.715.. Validation accuracy: 0.814\n",
      "Epoch 1/1.. Train loss: 0.981.. Validation loss: 0.709.. Validation accuracy: 0.808\n",
      "Epoch 1/1.. Train loss: 0.890.. Validation loss: 0.691.. Validation accuracy: 0.793\n",
      "Epoch 1/1.. Train loss: 1.073.. Validation loss: 0.674.. Validation accuracy: 0.814\n",
      "Epoch 1/1.. Train loss: 1.058.. Validation loss: 0.660.. Validation accuracy: 0.808\n",
      "Epoch 1/1.. Train loss: 1.155.. Validation loss: 0.630.. Validation accuracy: 0.838\n",
      "Epoch 1/1.. Train loss: 1.154.. Validation loss: 0.628.. Validation accuracy: 0.831\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs, steps, and running loss\n",
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "\n",
    "# Training the model and loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Loop over the training data\n",
    "    for images, labels in trainloader:\n",
    "        # Increment the steps\n",
    "        steps += 1\n",
    "        # Move the images and labels to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logps = model(images)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(logps, labels)\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        # Update the parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Increment the running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print the training loss every 'print_every' steps\n",
    "        if  steps % print_every == 0:\n",
    "            # Set the model to evaluation mode\n",
    "            model.eval()\n",
    "    \n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            # Loop over the validation data\n",
    "            for images, labels in vloader:\n",
    "                # Move the images and labels to the device\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass through the model\n",
    "                logps = model(images)\n",
    "                # Calculate the loss\n",
    "                batch_loss = criterion(logps, labels)\n",
    "                # Increment the test loss\n",
    "                test_loss += batch_loss.item()\n",
    "\n",
    "                # Calculate the accuracy \n",
    "\n",
    "                ps = torch.exp(logps)\n",
    "                # Get the prpobabilities and the top class\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                # Compare the top class with the labels\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                # Calculate the accuracy\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "                # Print the training and validation loss and accuracy\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {test_loss/len(vloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(vloader):.3f}\")\n",
    "            \n",
    "            # Reset the running loss\n",
    "            running_loss = 0\n",
    "            # Set the model back to training mode\n",
    "            model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
